# --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ ---
import os
import logging
import base64
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import tempfile

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
from docx import Document as WordDocument
from openpyxl import Workbook as ExcelWorkbook
from fpdf import FPDF

# –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã LangChain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage
from langchain.agents import AgentExecutor, create_tool_calling_agent, Tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferWindowMemory
from langchain_tavily import TavilySearch
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
load_dotenv()
if not all(key in os.environ for key in ["GOOGLE_API_KEY", "TELEGRAM_BOT_TOKEN", "TAVILY_API_KEY"]):
    raise ValueError("–û–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–π API –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ .env —Ñ–∞–π–ª–µ!")
print("‚úÖ –í—Å–µ –∫–ª—é—á–∏ API –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

# --- 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ---
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.5)
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
print("‚úÖ LLM –∏ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")

# --- 4. –ï–î–ò–ù–ê–Ø –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –ò –§–£–ù–ö–¶–ò–ò-–ò–ù–°–¢–†–£–ú–ï–ù–¢–´ ---
print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
persistent_storage_path = "/var/data/main_chroma_db"
main_db = Chroma(persist_directory=persistent_storage_path, embedding_function=embeddings)
retriever = main_db.as_retriever(search_kwargs={'k': 5})
print(f"‚úÖ –ï–¥–∏–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≥–æ—Ç–æ–≤–∞. –ó–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {main_db._collection.count()}")

# --- –§—É–Ω–∫—Ü–∏–∏-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã ---

def retrieve_from_memory(query: str) -> str:
    """–ò—â–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–ø—Ä–æ—Å –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ (–±–∞–∑–µ –∑–Ω–∞–Ω–∏–π)."""
    logger.info(f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 'retrieve_from_memory': –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
    docs = retriever.invoke(query)
    if not docs:
        return "–í –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
    return "\n".join([doc.page_content for doc in docs])

def research_and_learn(topic: str) -> str:
    """–ò—Å—Å–ª–µ–¥—É–µ—Ç —Ç–µ–º—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, —Å–æ–∑–¥–∞–µ—Ç —Å–∞–º–º–∞—Ä–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å."""
    logger.info(f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 'research_and_learn': –ù–∞—á–∏–Ω–∞—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–µ–º–µ: {topic}")
    
    planner_prompt = f"""–ú–Ω–µ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ç–µ–º—É '{topic}'. –°–æ–∑–¥–∞–π —Å–ø–∏—Å–æ–∫ –∏–∑ 3-4 –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–¥—Ç–µ–º –¥–ª—è –ø–æ–∏—Å–∫–∞. –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–º."""
    sub_topics_str = llm.invoke(planner_prompt).content
    sub_topics = [line.strip('- ').strip() for line in sub_topics_str.split('\n') if line.strip()]
    logger.info(f"–ü–ª–∞–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {sub_topics}")

    search = TavilySearch(max_results=2)
    full_raw_text = ""
    for sub_topic in sub_topics:
        logger.info(f"–ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –ø–æ–¥—Ç–µ–º–µ: {sub_topic}")
        try:
            search_results = search.invoke(f"{sub_topic} ({topic})")
            full_raw_text += f"### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ '{sub_topic}':\n" + "\n\n".join([res.get('content', '') for res in search_results]) + "\n\n"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ –ø–æ–¥—Ç–µ–º–µ '{sub_topic}': {e}")
            continue

    if not full_raw_text.strip():
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Ç–µ–º–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ."

    summarizer_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –ø–æ —Ç–µ–º–µ '{topic}'. –°–æ–∑–¥–∞–π –µ–¥–∏–Ω–æ–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–∞–º–º–∞—Ä–∏. –¢–ï–ö–°–¢:\n{full_raw_text}"""
    summary = llm.invoke(summarizer_prompt).content
    logger.info("–°–æ–∑–¥–∞–Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏.")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.create_documents([summary], metadatas=[{"source": f"Research on {topic}"}])
    main_db.add_documents(texts)
    
    logger.info(f"–°–∞–º–º–∞—Ä–∏ –ø–æ —Ç–µ–º–µ '{topic}' —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.")
    return f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ '{topic}' –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –º–æ–µ–π –ø–∞–º—è—Ç–∏."

def create_word_document(content: str) -> str:
    doc = WordDocument()
    doc.add_paragraph(content)
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".docx", prefix="report_")
    doc.save(temp_file.name)
    return f"–î–æ–∫—É–º–µ–Ω—Ç Word —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ –ø—É—Ç–∏: {temp_file.name}"

def quick_internet_search(query: str) -> str:
    """–î–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –Ω–µ —Ç—Ä–µ–±—É—é—â–∏—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç—å."""
    logger.info(f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 'quick_internet_search': –ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
    search = TavilySearch(max_results=3)
    try:
        results = search.invoke(query)
        return "\n\n".join([res.get('content', '') for res in results])
    except Exception as e:
        return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}"

def analyze_and_update_memory(query: str) -> str:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."""
    logger.info("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç 'analyze_and_update_memory': –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.")
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã
    all_docs = main_db.get(include=["metadatas"])
    if not all_docs or not all_docs.get('metadatas'):
        return "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞. –ù–µ—á–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å."

    # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–µ–º—ã –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    topics = list(set([meta['source'].replace("Research on ", "") for meta in all_docs['metadatas'] if 'source' in meta]))
    if not topics:
        return "–í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç —Ç–µ–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

    logger.info(f"–ù–∞–π–¥–µ–Ω—ã —Ç–µ–º—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {topics}")
    
    # –ü—Ä–æ—Å–∏–º LLM –≤—ã–±—Ä–∞—Ç—å –æ–¥–Ω—É, –Ω–∞–∏–±–æ–ª–µ–µ –Ω—É–∂–¥–∞—é—â—É—é—Å—è –≤ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ç–µ–º—É
    planner_prompt = f"""–í–æ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–º, –∫–æ—Ç–æ—Ä—ã–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {", ".join(topics)}.
    –ö–∞–∫–∞—è –∏–∑ —ç—Ç–∏—Ö —Ç–µ–º, –ø–æ-—Ç–≤–æ–µ–º—É, –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ –º–æ–≥–ª–∞ —É—Å—Ç–∞—Ä–µ—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –ø–æ–ª–∏—Ç–∏–∫–∞, –Ω–∞—É–∫–∞)?
    –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ–º –æ–¥–Ω–æ–π —Ç–µ–º—ã."""
    topic_to_update = llm.invoke(planner_prompt).content.strip()
    
    logger.info(f"–ê–Ω–∞–ª–∏—Ç–∏–∫ —Ä–µ—à–∏–ª –æ–±–Ω–æ–≤–∏—Ç—å —Ç–µ–º—É: {topic_to_update}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ
    return research_and_learn(topic_to_update)

# --- 5. –°–æ–∑–¥–∞–Ω–∏–µ –ï–î–ò–ù–û–ì–û –ê–ì–ï–ù–¢–ê –ò –ï–ì–û –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í ---
tools = [
    Tool(
        name="retrieve_from_memory",
        func=retrieve_from_memory,
        description="–ò—Å–ø–æ–ª—å–∑—É–π, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏. –≠—Ç–æ —Ç–≤–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∑–Ω–∞–Ω–∏–π –æ–± –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–Ω–µ–µ —Ç–µ–º–∞—Ö."
    ),
    Tool(
        name="quick_internet_search",
        func=quick_internet_search,
        description="–ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –æ —Ç–µ–∫—É—â–∏—Ö —Å–æ–±—ã—Ç–∏—è—Ö, –ø–æ–≥–æ–¥–µ, –Ω–æ–≤–æ—Å—Ç—è—Ö, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤ –ø–∞–º—è—Ç—å."
    ),
    Tool(
        name="research_and_learn",
        func=research_and_learn,
        description="–ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ù–û–í–û–ô, –æ–±—à–∏—Ä–Ω–æ–π —Ç–µ–º—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ø–∞–º—è—Ç—å. –ü—Ä–∏–º–µ–Ω—è–π, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä—è–º–æ –ø—Ä–æ—Å–∏—Ç '–∏—Å—Å–ª–µ–¥—É–π', '–Ω–∞–π–¥–∏'."
    ),
    Tool(
        name="analyze_and_update_memory",
        func=analyze_and_update_memory,
        description="–ò—Å–ø–æ–ª—å–∑—É–π, –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç '–∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–π –∑–Ω–∞–Ω–∏—è' –∏–ª–∏ '–æ–±–Ω–æ–≤–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é'. –≠—Ç–æ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å–∞–º –≤—ã–±–µ—Ä–µ—Ç, –∫–∞–∫—É—é —Ç–µ–º—É –æ–±–Ω–æ–≤–∏—Ç—å."
    ),
    Tool(
        name="create_word_document",
        func=create_word_document,
        description="–ò—Å–ø–æ–ª—å–∑—É–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ Microsoft Word (.docx)."
    ),
]
print(f"‚úÖ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –≥–æ—Ç–æ–≤—ã: {[tool.name for tool in tools]}")

system_prompt = """–¢—ã ‚Äî —É–º–Ω—ã–π –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ –æ—Ç–≤–µ—á–∞—è –Ω–∞ –µ–≥–æ –≤–æ–ø—Ä–æ—Å—ã –∏ –≤—ã–ø–æ–ª–Ω—è—è –∑–∞–¥–∞—á–∏, –∏—Å–ø–æ–ª—å–∑—É—è –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.

–¢–≤–æ–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
- **–ü–∞–º—è—Ç—å –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å:** –î–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –≤—Å–µ–≥–¥–∞ —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–π —Å–≤–æ—é –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å —Å –ø–æ–º–æ—â—å—é `retrieve_from_memory`.
- **–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –¥–ª—è —Ñ–∞–∫—Ç–æ–≤:** –ï—Å–ª–∏ –≤ –ø–∞–º—è—Ç–∏ –ø—É—Å—Ç–æ, –∞ –≤–æ–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –±—ã—Å—Ç—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –æ —Ç–µ–∫—É—â–∏—Ö —Å–æ–±—ã—Ç–∏—è—Ö (–ø–æ–≥–æ–¥–∞, –Ω–æ–≤–æ—Å—Ç–∏), –∏—Å–ø–æ–ª—å–∑—É–π `quick_internet_search`.
- **–ì–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –∫–æ–º–∞–Ω–¥–µ:** –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä—è–º–æ –ø—Ä–æ—Å–∏—Ç '–∏—Å—Å–ª–µ–¥—É–π' –∏–ª–∏ '—Å–æ—Ö—Ä–∞–Ω–∏', –∏—Å–ø–æ–ª—å–∑—É–π `research_and_learn`.
- **–ê–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–º–∞–Ω–¥–µ:** –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç '–æ–±–Ω–æ–≤–∏' –∏–ª–∏ '–∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–π', –∏—Å–ø–æ–ª—å–∑—É–π `analyze_and_update_memory`.
- **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–æ –∑–∞–ø—Ä–æ—Å—É:** –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä—è–º–æ –æ–± —ç—Ç–æ–º –ø—Ä–æ—Å–∏—Ç.
- **–ß–µ—Å—Ç–Ω–æ—Å—Ç—å:** –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º.
- **–ö–æ–Ω—Ç–µ–∫—Å—Ç:** –í—Å–µ–≥–¥–∞ —É—á–∏—Ç—ã–≤–∞–π –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –¥–∏–∞–ª–æ–≥–µ.
"""

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder("agent_scratchpad"),
])

agent = create_tool_calling_agent(llm, tools, prompt)
memory = ConversationBufferWindowMemory(k=8, memory_key="chat_history", return_messages=True)
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    memory=memory,
    verbose=True,
    handle_parsing_errors=True
)
print("‚úÖ –ï–¥–∏–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω.")

# --- 6. –§—É–Ω–∫—Ü–∏–∏-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è Telegram ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    context.user_data['chat_history'] = []
    await update.message.reply_text('–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –Ø –ø–æ–º–Ω—é –Ω–∞—à –¥–∏–∞–ª–æ–≥. –ó–∞–¥–∞–π—Ç–µ –º–Ω–µ –≤–æ–ø—Ä–æ—Å.')

async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_query = update.message.text
    logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ –∑–∞–¥–∞—á–∞: '{user_query}'")
    await update.message.reply_text('–ü—Ä–∏—Å—Ç—É–ø–∞—é –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∑–∞–¥–∞—á–∏...')
    
    try:
        chat_history = context.user_data.get('chat_history', [])
        result = agent_executor.invoke({"input": user_query, "chat_history": chat_history})
        
        context.user_data['chat_history'] = result['chat_history']

        response_text = result["output"]
        if response_text.startswith("–î–æ–∫—É–º–µ–Ω—Ç") and '.docx' in response_text:
            try:
                file_path = response_text.split(":")[-1].strip()
                if os.path.exists(file_path):
                    await context.bot.send_document(chat_id=update.effective_chat.id, document=open(file_path, 'rb'))
                    os.remove(file_path)
                else:
                    await update.message.reply_text("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}", exc_info=True)
                await update.message.reply_text(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç.")
        else:
            await update.message.reply_text(response_text)
            logger.info("–û—Ç–ø—Ä–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç.")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)
        await update.message.reply_text(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞.")

async def handle_photo_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    logger.info("–ü–æ–ª—É—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
    await update.message.reply_text('–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...')
    try:
        photo_file = await update.message.photo[-1].get_file()
        photo_bytes = await photo_file.download_as_bytearray()
        user_caption = update.message.caption or "–û–ø–∏—à–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ."
        
        base64_image = base64.b64encode(photo_bytes).decode("utf-8")
        
        message_payload = HumanMessage(
            content=[
                {"type": "text", "text": user_caption},
                {"type": "image_url", "image_url": f"data:image/jpeg;base64,{base64_image}"},
            ]
        )
        response = llm.invoke([message_payload])
        await update.message.reply_text(response.content)
        logger.info("–û—Ç–ø—Ä–∞–≤–ª–µ–Ω –æ—Ç–≤–µ—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}", exc_info=True)
        await update.message.reply_text(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")

# --- 7. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ ---
def main() -> None:
    application = Application.builder().token(os.environ["TELEGRAM_BOT_TOKEN"]).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_message))
    application.add_handler(MessageHandler(filters.PHOTO, handle_photo_message))
    print("üöÄ –ó–∞–ø—É—Å–∫–∞—é Telegram-–±–æ—Ç–∞...")
    application.run_polling()

if __name__ == '__main__':
    main()