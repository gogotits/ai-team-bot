# --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ ---
import os
import logging
import base64
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
import tempfile

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
from docx import Document as WordDocument
from openpyxl import Workbook as ExcelWorkbook
from fpdf import FPDF

# –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã LangChain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain.agents import AgentExecutor, create_tool_calling_agent, Tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferWindowMemory
from langchain_tavily import TavilySearch
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
load_dotenv()
if not all(key in os.environ for key in ["GOOGLE_API_KEY", "TELEGRAM_BOT_TOKEN", "TAVILY_API_KEY"]):
    raise ValueError("–û–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–π API –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ .env —Ñ–∞–π–ª–µ!")
print("‚úÖ –í—Å–µ –∫–ª—é—á–∏ API –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

# --- 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò-–∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ ---
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", temperature=0.5)
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
print("‚úÖ LLM –∏ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")

# --- 4. –ï–î–ò–ù–ê–Ø –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô ---
print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –µ–¥–∏–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
persistent_storage_path = "/var/data/main_chroma_db"
main_db = Chroma(persist_directory=persistent_storage_path, embedding_function=embeddings)
retriever = main_db.as_retriever(search_kwargs={'k': 5})
print(f"‚úÖ –ï–¥–∏–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≥–æ—Ç–æ–≤–∞. –ó–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {main_db._collection.count()}")

# --- 5. –§–£–ù–ö–¶–ò–ò-–≠–ö–°–ü–ï–†–¢–´ ---

def research_and_learn(topic: str) -> str:
    logger.info(f"–≠–∫—Å–ø–µ—Ä—Ç 'DeepResearcher': –ù–∞—á–∏–Ω–∞—é –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ —Ç–µ–º–µ: {topic}")
    try:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –Ø–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–µ–º API-–∫–ª—é—á –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        search = TavilySearch(max_results=3, api_key=os.environ.get("TAVILY_API_KEY"))
        search_results = search.invoke(topic)
        raw_text = "\n\n".join([result.get('content', '') for result in search_results])
        if not raw_text.strip(): return "–ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."
        
        summarizer_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –ø–æ —Ç–µ–º–µ '{topic}'. –°–æ–∑–¥–∞–π –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–∞–º–º–∞—Ä–∏."""
        summary = llm.invoke(summarizer_prompt).content
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.create_documents([summary], metadatas=[{"source": f"Research on {topic}"}])
        main_db.add_documents(texts)
        
        return f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ '{topic}' –±—ã–ª–∞ —É—Å–ø–µ—à–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –º–æ–µ–π –ø–∞–º—è—Ç–∏."
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ä–∞–±–æ—Ç–µ 'DeepResearcher': {e}", exc_info=True)
        return f"–í –ø—Ä–æ—Ü–µ—Å—Å–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}"

def retrieve_from_memory(query: str) -> str:
    logger.info(f"–≠–∫—Å–ø–µ—Ä—Ç 'MemoryArchivist': –ü–æ–∏—Å–∫ –≤ –ø–∞–º—è—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
    docs = retriever.invoke(query)
    if not docs:
        return "–í –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
    return "\n".join([doc.page_content for doc in docs])

def quick_internet_search(query: str) -> str:
    logger.info(f"–≠–∫—Å–ø–µ—Ä—Ç 'FactChecker': –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}")
    try:
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –Ø–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–µ–º API-–∫–ª—é—á –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
        search = TavilySearch(max_results=1, api_key=os.environ.get("TAVILY_API_KEY"))
        results = search.invoke(query)
        answer = results[0].get('answer')
        if answer:
            return answer
        return results[0].get('content', '–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.')
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –±—ã—Å—Ç—Ä–æ–º –ø–æ–∏—Å–∫–µ: {e}"

def create_document(input_str: str) -> str:
    logger.info(f"–≠–∫—Å–ø–µ—Ä—Ç 'Secretary': –ü–æ–ª—É—á–µ–Ω–∞ –∑–∞–¥–∞—á–∞ –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.")
    try:
        parts = input_str.split('|')
        if len(parts) != 2:
            return "–û—à–∏–±–∫–∞: –Ω–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ '—Ç–µ–∫—Å—Ç|—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞' (word, excel, pdf)."
        content, doc_type = parts[0].strip(), parts[1].strip().lower()

        if doc_type == 'word':
            doc = WordDocument()
            doc.add_paragraph(content)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".docx", prefix="report_")
            doc.save(temp_file.name)
            return f"–î–æ–∫—É–º–µ–Ω—Ç Word —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {temp_file.name}"
        elif doc_type == 'excel':
            wb = ExcelWorkbook()
            ws = wb.active
            for line in content.split('\n'):
                ws.append(line.split(','))
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx", prefix="table_")
            wb.save(temp_file.name)
            return f"–î–æ–∫—É–º–µ–Ω—Ç Excel —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {temp_file.name}"
        elif doc_type == 'pdf':
            pdf = FPDF()
            pdf.add_page()
            pdf.add_font('DejaVu', '', '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf', uni=True)
            pdf.set_font('DejaVu', '', 12)
            pdf.multi_cell(0, 10, content)
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf", prefix="document_")
            pdf.output(temp_file.name)
            return f"PDF –¥–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: {temp_file.name}"
        else:
            return "–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∏–ø—ã: word, excel, pdf."
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}"

# --- 6. –°–û–ó–î–ê–ù–ò–ï –ê–ì–ï–ù–¢–ê-–°–û–ë–ï–°–ï–î–ù–ò–ö–ê ---
conversational_tools = [
    Tool(
        name="MemoryArchivist",
        func=retrieve_from_memory,
        description="–ò—Å–ø–æ–ª—å–∑—É–π, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –≤ —Å–≤–æ–µ–π –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏."
    ),
]
system_prompt_conv = """–¢—ã ‚Äî –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –±–µ—Å–µ–¥—É —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.
–ï—Å–ª–∏ —Ç—ã –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Å–≤–æ—é –ø–∞–º—è—Ç—å (`MemoryArchivist`), —Å–¥–µ–ª–∞–π —ç—Ç–æ.
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–µ—Ç –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–π —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –≤–µ–∂–ª–∏–≤–æ —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å, –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –µ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ–º–∞–Ω–¥—ã "–∏—Å—Å–ª–µ–¥—É–π" –∏–ª–∏ "–ø–æ—Å–º–æ—Ç—Ä–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"."""

prompt_conv = ChatPromptTemplate.from_messages([
    ("system", system_prompt_conv),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
    MessagesPlaceholder("agent_scratchpad"),
])
memory = ConversationBufferWindowMemory(k=10, memory_key="chat_history", return_messages=True)
conversational_agent_executor = AgentExecutor(
    agent=create_tool_calling_agent(llm, conversational_tools, prompt_conv),
    tools=conversational_tools,
    memory=memory,
    verbose=True,
    handle_parsing_errors=True
)
print("‚úÖ –ê–≥–µ–Ω—Ç-–°–æ–±–µ—Å–µ–¥–Ω–∏–∫ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ.")

# --- 7. –§—É–Ω–∫—Ü–∏–∏-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è Telegram ---
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    if 'chat_history' in context.user_data:
        del context.user_data['chat_history']
    memory.clear()
    await update.message.reply_text('–ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞. –ó–∞–¥–∞–π—Ç–µ –º–Ω–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –¥–∞–π—Ç–µ –∫–æ–º–∞–Ω–¥—É.')

async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    user_query = update.message.text
    logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ –∑–∞–¥–∞—á–∞: '{user_query}'")
    await update.message.reply_text('–ü—Ä–∏—Å—Ç—É–ø–∞—é –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é –∑–∞–¥–∞—á–∏...')
    
    response_text = ""
    
    try:
        # –ñ–ï–°–¢–ö–ò–ô –î–ò–°–ü–ï–¢–ß–ï–† –ù–ê –û–°–ù–û–í–ï –ö–û–ú–ê–ù–î
        if user_query.lower().startswith("–∏—Å—Å–ª–µ–¥—É–π"):
            topic = user_query[len("–∏—Å—Å–ª–µ–¥—É–π"):].strip()
            response_text = research_and_learn(topic)
        elif user_query.lower().startswith("–ø–æ—Å–º–æ—Ç—Ä–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"):
            topic = user_query[len("–ø–æ—Å–º–æ—Ç—Ä–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"):].strip()
            response_text = quick_internet_search(topic)
        elif user_query.lower().startswith("–ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏"):
            topic = user_query[len("–ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø–∞–º—è—Ç–∏"):].strip()
            response_text = retrieve_from_memory(topic)
        elif user_query.lower().startswith("—Å–æ–∑–¥–∞–π –¥–æ–∫—É–º–µ–Ω—Ç"):
            topic = user_query[len("—Å–æ–∑–¥–∞–π –¥–æ–∫—É–º–µ–Ω—Ç"):].strip()
            response_text = create_document(topic)
        # –£–¥–∞–ª—è–µ–º '–∞–Ω–∞–ª–∏—Ç–∏–∫–∞' –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        # elif user_query.lower().startswith("–∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–π"):
        #     topic = user_query[len("–∞–∫—Ç—É–∞–ª–∏–∑–∏—Ä—É–π"):].strip()
        #     response_text = analyze_and_update_memory(topic)
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –∫–æ–º–∞–Ω–¥—ã, –≤—ã–∑—ã–≤–∞–µ–º –∞–≥–µ–Ω—Ç–∞-—Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞
            chat_history = context.user_data.get('chat_history', [])
            result = conversational_agent_executor.invoke({"input": user_query, "chat_history": chat_history})
            response_text = result["output"]
            context.user_data['chat_history'] = result['chat_history']

        if response_text.startswith("–î–æ–∫—É–º–µ–Ω—Ç") and ('.docx' in response_text or '.xlsx' in response_text or '.pdf' in response_text):
            try:
                file_path = response_text.split(":")[-1].strip()
                if os.path.exists(file_path):
                    await context.bot.send_document(chat_id=update.effective_chat.id, document=open(file_path, 'rb'))
                    os.remove(file_path)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}", exc_info=True)
                await update.message.reply_text(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç.")
        else:
            await update.message.reply_text(response_text)
            logger.info("–û—Ç–ø—Ä–∞–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç.")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞: {e}", exc_info=True)
        await update.message.reply_text(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞.")

async def handle_photo_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    logger.info("–ü–æ–ª—É—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
    await update.message.reply_text('–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...')
    try:
        photo_file = await update.message.photo[-1].get_file()
        photo_bytes = await photo_file.download_as_bytearray()
        user_caption = update.message.caption or "–û–ø–∏—à–∏ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ."
        
        base64_image = base64.b64encode(photo_bytes).decode("utf-8")
        
        message_payload = HumanMessage(content=[{"type": "text", "text": user_caption}, {"type": "image_url", "image_url": f"data:image/jpeg;base64,{base64_image}"},])
        response = llm.invoke([message_payload])
        await update.message.reply_text(response.content)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}", exc_info=True)
        await update.message.reply_text(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")

# --- 8. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞ ---
def main() -> None:
    application = Application.builder().token(os.environ["TELEGRAM_BOT_TOKEN"]).build()
    application.add_handler(CommandHandler("start", start))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_message))
    application.add_handler(MessageHandler(filters.PHOTO, handle_photo_message))
    print("üöÄ –ó–∞–ø—É—Å–∫–∞—é Telegram-–±–æ—Ç–∞ —Å –∂–µ—Å—Ç–∫–∏–º –¥–∏—Å–ø–µ—Ç—á–µ—Ä–æ–º...")
    application.run_polling()

if __name__ == '__main__':
    main()